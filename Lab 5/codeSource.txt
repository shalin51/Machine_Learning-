
public class ReadWriteOperation {
    static String ReadFile(String path, Charset encoding) throws IOException
    {
        byte[] encoded = Files.readAllBytes(Paths.get(path));
        return new String(encoded, encoding);
    }

    static void WriteToFile(String lst, String path)
    {
        try{

            File file = new File(path);

            if (!file.exists()) {
                file.createNewFile();
            }

            FileWriter writer= new FileWriter(path,true);
                writer.write(lst+"\n");
            writer.close();
        } catch (IOException e) {

        }
    }
 public Map<String,String> GetNERR(CoreMap sentence) {
        Map<String,String> NERList = new HashMap<>();
        for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
            String tokenNumber=token.toString().split("-")[1];
            String tokenString=token.toString().split("-")[0];
            NERList.put(tokenString,token.get(CoreAnnotations.NamedEntityTagAnnotation.class));
        }

        return NERList;
    }
        static List<String> GetListOfFileFromFolder(String directoryPath){
            List<String> allFiles=new ArrayList<>();
            try (Stream<Path> paths = Files.walk(Paths.get(directoryPath))) {
                paths
                           .forEach(new Consumer<Path>() {
                            @Override
                            public void accept(Path file) {
                                allFiles.add(file.toString());
                            }
                        });
            }catch (IOException io){
                System.out.println(io);
            }
            return allFiles;
        }

    public List<String> GetNER(CoreMap sentence,String sentenceNumber) {
        List<String> NERList = new ArrayList<>();
            for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
                String tokenNumber=token.toString().split("-")[1];
                String tokenString=token.toString().split("-")[0];
                NERList.add("("+sentenceNumber+","+tokenNumber+","+tokenString+","+token.get(CoreAnnotations.NamedEntityTagAnnotation.class)+")");
            }

        return NERList;
    }
        static void RemoveEmptyLines() {
            try {
                FileReader fr = new FileReader("data/WholeDataSet.txt");
                BufferedReader br = new BufferedReader(fr);
                FileWriter fw = new FileWriter("data/AlltheSentences.txt");
                String line;

                while ((line = br.readLine()) != null) {
                    line = line.trim(); // remove leading and trailing whitespace
                    if (!line.equals("")) // don't write out blank lines
                    {
                        fw.write(line, 0, line.length());
                    }
                }
                fr.close();
                fw.close();
            }catch (IOException io)
            {

            }
        }


   static void ReadAllFileFromFolder(String inputDirectoryPath,String outputPath) {

       List<String> allFile = GetListOfFileFromFolder(inputDirectoryPath);
       for (String fileName : allFile) {
           try {
               WriteToFile(ReadFile(fileName, StandardCharsets.UTF_8), outputPath);
           } catch (IOException io) {

           }
       }
   }

    static void CreateandWriteToCSV(List<NLPData> nlpdata, String fileName){

          String COMMA = ",";
          String NEW_LINE = "\n";

          String headers="Token,Lamma,POS,NER";

          FileWriter fWriter=null ;
        try {

            fWriter = new FileWriter(fileName);
            fWriter.append(headers);

            fWriter.append(NEW_LINE);

            for (NLPData nlp : nlpdata) {
                fWriter.append(nlp.token);

                fWriter.append(COMMA);

                fWriter.append(nlp.lemma);

                fWriter.append(COMMA);

                fWriter.append(nlp.POS);

                fWriter.append(COMMA);

                fWriter.append(nlp.NER);

                fWriter.append(COMMA);

                fWriter.append(NEW_LINE);

            }

            System.out.println("CSV file was created successfully !!!");

        } catch (Exception e) {

            System.out.println("Error in CsvFileWriter !!!");

            e.printStackTrace();

        } finally {

            try {

                fWriter.flush();

                fWriter.close();

            } catch (IOException e) {

                System.out.println("Error while flushing/closing fileWriter !!!");

                e.printStackTrace();

            }

        }



    }
}



public class Tutorial2JavaMain {
    public static void main(String[] args) {

        try {

            String inputFileName = "E:\\Knowledge Discovery Management\\Datasets\\WikiRef_dataset\\WikiRef_dataset\\WikiRef220\\barack.obama.1.txt";
            String outputFileName = "E:\\Knowledge Discovery Management\\Tutorials\\ScalaTutorial1\\output\\t2output.txt";
            String processedData = "E:\\Knowledge Discovery Management\\Tutorials\\ScalaTutorial1\\output\\processedData.txt";
            String lemmaOutput = "E:\\Knowledge Discovery Management\\Tutorials\\Tutorial2\\output\\lemmaOutput.txt";
            String textOutput = "E:\\Knowledge Discovery Management\\Tutorials\\Tutorial2\\output\\lemmaOutput.txt";
            String POSOutput = "E:\\Knowledge Discovery Management\\Tutorials\\Tutorial2\\output\\lemmaOutput.txt";
            String NEROutput = "E:\\Knowledge Discovery Management\\Tutorials\\Tutorial2\\output\\lemmaOutput.txt";
            String treeOutput = "E:\\Knowledge Discovery Management\\Tutorials\\Tutorial2\\output\\treeOutput.txt";
            String semanticGraphOutput = "E:\\Knowledge Discovery Management\\Tutorials\\Tutorial2\\output\\semanticGraphOutput.txt";
            NLPOperations nlp=new NLPOperations();

            // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
            Properties prop = new Properties();
            prop.setProperty("annotators", "tokenize, ssplit, pos, lemma, ner, parse, dcoref");
            StanfordCoreNLP pipeline = new StanfordCoreNLP(prop);

            // reading text from file
            String text = ReadFile(inputFileName,StandardCharsets.UTF_8);

            // create an empty Annotation just with the given text
            Annotation document = new Annotation(text);

            // run all Annotators on this text
            pipeline.annotate(document);

            List<CoreMap> sentences = document.get(CoreAnnotations.SentencesAnnotation.class);
            Set<String> lemmas= nlp.GetLemmas(sentences);



        } catch (IOException io) {
            System.out.print(io.toString());
        }
    }


    static String ReadFile(String path, Charset encoding) throws IOException
    {
        byte[] encoded = Files.readAllBytes(Paths.get(path));
        return new String(encoded, encoding);
    }

    static String ReadProcessedData(String outputFileName,String question){
        String ans="Not able to find answer!!!!";
        String[] splitedQuestion = question.split("\\s+");
        String line="";
        String que="who is President";
        boolean hasWord=false;
        try {
            FileReader fileReader =
                    new FileReader(outputFileName);

            // Always wrap FileReader in BufferedReader.
            BufferedReader bufferedReader =
                    new BufferedReader(fileReader);
            List<String> ignoreList = GetIgnoreList();
            String[] questionWords = question.split(" ");

            while ((line = bufferedReader.readLine()) != null) {
                String[] anws= line.split("\"");
                for(String queWord:questionWords)
                {
                    for(String anw:anws)
                    {
                        if(anw.contains(queWord))
                        {
                            ans=anw;
                            break;
                        }
                    }
                }


            }
        }
        catch (IOException io) {
        }
        finally {
            return ans;
        }
    }

    static void WriteToFile(Set<String> lst, String path)
    {
        try{
            FileWriter writer= new FileWriter(path);
            for(String line:lst){
                writer.write(line);
            }
            writer.close();
        } catch (IOException e) {
            // do something
        }
    }

    


public class OpenIEOper {
    public static String GetTriplet(String sen) {

        Document docs = new Document(sen);
        String triplet="";
        for (Sentence sent : docs.sentences()) {
            Collection<Quadruple<String, String, String, Double>> lemma=sent.openie();
            for (Quadruple<String, String, String, Double> lem:lemma) {
                if (lem.fourth>0.5) {
                    triplet += lem.toString();
                    System.out.println(triplet);
                }
                else {
                    triplet="Rephrase the question please.";
                }
            }
        }
        return triplet;
    }

}
static void WriteToFile(String str, String path)
    {
        try{
            FileWriter writer= new FileWriter(path);
            writer.write(str);
            writer.close();
        } catch (IOException e) {
            // do something
        }
    }

    static List<String> GetIgnoreList(){
        List<String> ignoreList= new ArrayList<>();
        ignoreList.add("who");
        ignoreList.add("what");
        ignoreList.add("how");
        ignoreList.add("is");
        ignoreList.add("that");
        ignoreList.add("are");
        ignoreList.add("why");
        ignoreList.add("the");
        ignoreList.add("he");
        ignoreList.add("of");
        ignoreList.add("the");
        ignoreList.add("in");
        ignoreList.add("from");
        ignoreList.add("his");
        ignoreList.add("her");
        ignoreList.add("a");
        return ignoreList;
    }

}

